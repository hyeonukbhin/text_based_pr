{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttttt\n",
      "0.0\n",
      "None\n",
      "end======\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple\n",
    "from pprintpp import pprint\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]  # header 제외\n",
    "    return data\n",
    "\n",
    "'''\n",
    "#AUTHID\tSTATUS\tsEXT\tsNEU\tsAGR\tsCON\tsOPN\tcEXT\tcNEU\tcAGR\tcCON\tcOPN\tDATE\tNETWORKSIZE\tBETWEENNESS\tNBETWEENNESS\tDENSITY\tBROKERAGE\tNBROKERAGE\tTRANSITIVITY\n",
    "'''\n",
    "\n",
    "\n",
    "def read_df(filename):\n",
    "    # df = pd.read_csv(filename, sep=',', na_values=\".\",index_col=0, encoding = \"ISO-8859-1\")\n",
    "    df = pd.read_csv(filename, sep=',', na_values=\".\", encoding = \"ISO-8859-1\")\n",
    "    return df\n",
    "\n",
    "train_df = read_df('../../dataset/mypersonality.csv')\n",
    "# test_data = read_data('../ratings_test.txt')\n",
    "# df_comp = train_df[['STATUS','sEXT','sNEU','sAGR','sCON','sOPN']]\n",
    "df_comp = train_df[[\"#AUTHID\", 'STATUS','sEXT','sNEU','sAGR','sCON','sOPN']]\n",
    "# df_comp = train_df[['STATUS','sEXT','sNEU','sAGR','sCON','sOPN']]\n",
    "\n",
    "# df_comp = df_comp[80:120]\n",
    "# authid_list = list(set(df_comp.index.values))\n",
    "authid_list = list(set(df_comp[\"#AUTHID\"]))\n",
    "# print(len(authid_list))\n",
    "\n",
    "THRES_ID = 3\n",
    "BOUND_TT = 180\n",
    "\n",
    "for authid in authid_list:\n",
    "    df_id = df_comp[df_comp['#AUTHID'] == authid]\n",
    "\n",
    "    count = len(df_id.index)\n",
    "\n",
    "    if count < THRES_ID:\n",
    "        df_comp = df_comp[df_comp['#AUTHID'] != authid]\n",
    "\n",
    "# print(df_comp)\n",
    "# print(authid_list)\n",
    "# df_comp[\"\"]\n",
    "\n",
    "authid_list = list(set(df_comp[\"#AUTHID\"]))\n",
    "# print(len(authid_list))\n",
    "\n",
    "train_authid_list = authid_list[:BOUND_TT]\n",
    "test_authid_list = authid_list[BOUND_TT:]\n",
    "\n",
    "\n",
    "# print(len(train_authid_list))\n",
    "# print(len(test_authid_list))\n",
    "import ast\n",
    "import re\n",
    "def cleanText(readData):\n",
    "    #텍스트에 포함되어 있는 특수 문자 제거\n",
    "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', readData)\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "# tokenized_text = [nltk.word_tokenize(cleanText(sentence)) for sentence in test]\n",
    "# test_result = [t for d in tokenized_text for t in d]\n",
    "#\n",
    "# print(tokenized_text)\n",
    "# print(test_result)\n",
    "# print(\"asdfasdf\")\n",
    "# test_result = [t for d in status_list for t in d]\n",
    "\n",
    "# print(test_result)\n",
    "\n",
    "def tokenize(df, authid):\n",
    "    # df_id = df.loc[authid]\n",
    "    df_id = df_comp[df_comp['#AUTHID'] == authid]\n",
    "\n",
    "    # print(df_id[\"STATUS\"])\n",
    "    # print(len(df_id[\"STATUS\"]))\n",
    "    # print(\"tteste\")\n",
    "    status_list = df_id[\"STATUS\"].tolist()\n",
    "    # train_docs = df_train_docs[\"sentence\"].to_list()\n",
    "    cleaned_tokens_2d_list = [nltk.word_tokenize(cleanText(sentence)) for sentence in status_list]\n",
    "    splited_token_list = [t for d in cleaned_tokens_2d_list for t in d]\n",
    "\n",
    "\n",
    "    # status_list = [ast.literal_eval(cleanText(t)) for t in status_list]\n",
    "    # token_list = [t for d in status_list for t in d]\n",
    "    p_score_list = df_id[['sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN']].iloc[0].values.tolist()\n",
    "    # print(token_list)\n",
    "    ext_score = df_id['sEXT'].iloc[0]\n",
    "    neu_score = df_id['sNEU'].iloc[0]\n",
    "    agr_score = df_id['sAGR'].iloc[0]\n",
    "    con_score = df_id['sCON'].iloc[0]\n",
    "    opn_score = df_id['sOPN'].iloc[0]\n",
    "\n",
    "    # p_score_list = [ext_score,neu_score,agr_score,con_score,opn_score]\n",
    "    return splited_token_list, p_score_list\n",
    "\n",
    "\n",
    "\n",
    "train_docs = [(tokenize(df_comp, authid)) for authid in train_authid_list]\n",
    "test_docs = [(tokenize(df_comp, authid)) for authid in test_authid_list]\n",
    "\n",
    "\n",
    "# pprint(train_docs)\n",
    "\n",
    "print(\"ttttt\")\n",
    "\n",
    "\n",
    "\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "# # 여기서는 15만개 training documents 전부 사용함\n",
    "\n",
    "\n",
    "def act_fuction(x):\n",
    "    if x > 0.5:\n",
    "        y = 1\n",
    "    else:\n",
    "        y = 0\n",
    "    return y\n",
    "\n",
    "tagged_train_docs = [TaggedDocument(d, [str(act_fuction(c[0]))]) for d, c in train_docs]\n",
    "#\n",
    "# pprint(tagged_train_docs)\n",
    "#\n",
    "tagged_test_docs = [TaggedDocument(d, [str(act_fuction(c[0]))]) for d, c in test_docs]\n",
    "\n",
    "# pprint(tagged_train_docs[:20])\n",
    "\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# data = [\"I love machine learning. Its awesome.\",\n",
    "#         \"I love coding in python\",\n",
    "#         \"I love building chatbots\",\n",
    "#         \"they chat amagingly well\"]\n",
    "#\n",
    "# tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "\n",
    "\n",
    "# pprint(tagged_data)\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "#doc2vec parameters\n",
    "\n",
    "vector_size = 300\n",
    "\n",
    "window_size = 15\n",
    "\n",
    "word_min_count = 2\n",
    "\n",
    "sampling_threshold = 1e-5\n",
    "\n",
    "negative_size = 5\n",
    "\n",
    "train_epoch = 10\n",
    "\n",
    "dm = 1 # {0:dbow, 1:dmpv}\n",
    "\n",
    "worker_count = cores  # number of parallel processes\n",
    "\n",
    "\n",
    "import gensim\n",
    "# from gensim.models import doc2vec\n",
    "# 사전 구축\n",
    "\n",
    "model = gensim.models.Doc2Vec(vector_size=300,\n",
    "                              window=5,\n",
    "                              seed=1234,\n",
    "                              negative=5,\n",
    "                              min_count=5,\n",
    "                              workers=worker_count,\n",
    "                              alpha=0.025,\n",
    "                              min_alpha=0.025,\n",
    "                              epochs=10,\n",
    "                              compute_loss=True)\n",
    "model.build_vocab(tagged_train_docs)\n",
    "model.train(tagged_train_docs, epochs=model.epochs, total_examples=model.corpus_count)\n",
    "print(model.running_training_loss)\n",
    "# print(model.compute_loss)\n",
    "print(model.comment)\n",
    "\n",
    "# print(\"\\nEvaluating %s\" % model)\n",
    "# err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)\n",
    "# error_rates[str(model)] = err_rate\n",
    "# print(\"\\n%f %s\\n\" % (err_rate, model))\n",
    "# print(model.get_latest_training_loss())\n",
    "# tsne\n",
    "\n",
    "# model.get\n",
    "# doc_vectorizer = doc2vec.Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.025, seed=1234, total_examples=doc_vectorizer.corpus_count)\n",
    "# doc_vectorizer.build_vocab(tagged_train_docs)\n",
    "# # Train document vectors!\n",
    "# for epoch in range(10):\n",
    "#     doc_vectorizer.train(tagged_train_docs)\n",
    "#     doc_vectorizer.alpha -= 0.002  # decrease the learning rate\n",
    "#     doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay\n",
    "# To save\n",
    "\n",
    "model.save(\"personality_doc2vec.model\")\n",
    "\n",
    "print(\"end======\")\n",
    "\n",
    "# document처리할것\n",
    "#\n",
    "# document1 id에 따라서\n",
    "# document2 personality score에 따라서\n",
    "#\n",
    "#\n",
    "# 같은 성격 점수를 갖고 있는 사람들끼리\n",
    "#\n",
    "# 같은 성격 클래스 갖고 있는 사람들 끼리 클래스 처리 할것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
